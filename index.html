<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Discover our cross-modal self-supervised learning framework for vision-based estimation of environmental physical parameters, enhancing robotic locomotion and navigation. Validated with the ANYmal robot, our approach predicts friction and stiffness from images, bridging the gap between simulation and real-world deployment.">
  <meta property="og:title" content="Identifying Terrain Physical Parameters from Vision"/>
  <meta property="og:description" content="Explore our innovative cross-modal self-supervised learning framework for estimating environmental physical parameters from vision, enhancing robotic locomotion and navigation."/>
  <meta property="og:url" content="https://bit.ly/3Xo5AA8"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/thumbnail.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Identifying Terrain Physical Parameters from Vision">
  <meta name="twitter:description" content="Explore our innovative cross-modal self-supervised learning framework for estimating environmental physical parameters from vision, enhancing robotic locomotion and navigation.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/thumbnail.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="cross-modal self-supervised learning, vision-based estimation, robotic locomotion, navigation, ANYmal robot, physical parameters, friction, stiffness, simulation, real-world deployment">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Identifying Terrain Physical Parameters from Vision - Towards Physical-Parameter-Aware Locomotion and Navigation</title>
  <link rel="icon" type="image/x-icon" href="static/images/JQ.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Identifying Terrain Physical Parameters from Vision</h1>
            <h2 class="subtitle is-3">Towards Physical-Parameter-Aware Locomotion and Navigation</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jiaqi-chen-2b07a3280/" target="_blank">Jiaqi Chen</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/jonas-frey-3a3702175/" target="_blank">Jonas Frey</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=S5U9JpkAAAAJ&hl" target="_blank">Ruyi Zhou</a>,</span>
                    <span class="author-block">
                      <a href="https://www.linkedin.com/in/takahiro-miki-a6a065218/" target="_blank">Takahiro Miki</a>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.de/citations?user=b-JF-UIAAAAJ&hl" target="_blank">Georg Martius</a>,</span>
                        <span class="author-block">
                          <a href="https://www.linkedin.com/in/marco-hutter/" target="_blank">Marco Hutter</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Robotic Systems Lab, ETH Zurich | Autonomous Learning Group, MPI for Intelligent Systems<br>accepted for IEEE Robotics and Automation Letters (RA-L) 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2408.16567.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="xxx" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Coming soon</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2408.16567" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Suppl_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Supplementary Video of the Paper 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Identifying the physical properties of the surrounding environment is essential for robotic locomotion and navigation to deal with non-geometric hazards, such as slippery and deformable terrains.
It would be of great benefit for robots to anticipate these extreme physical properties before contact; however, estimating environmental physical parameters from vision is still an open challenge. 
Animals can achieve this by using their prior experience and knowledge of what they have seen and how it felt. 
In this work, we propose a cross-modal self-supervised learning framework for vision-based environmental physical parameter estimation, which paves the way for future physical properties-aware locomotion and navigation. We bridge the gap between existing policies trained in simulation and identification of physical terrain parameters from vision. We propose to train a physical decoder in simulation to predict friction and stiffness from multi-modal input. The trained network allows the labeling of real-world images with physical parameters in a self-supervised manner to further train a visual network during deployment, which can densely predict the friction and stiffness from image data. We validate our physical decoder in simulation and the real world using a quadruped ANYmal robot, outperforming an existing baseline method. We show that our visual network can predict the physical properties in indoor and outdoor experiments while allowing fast adaptation to new environments. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <!-- System Overview Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3">System Overview</h1>
            <div class="item">
              <img src="static/images/fig1.png" alt="System Overview"/>
              <p class="has-text-centered">
                Overview of the two-stage self-supervised terrain physical parameter learning framework. A physical decoder in twin structure is trained in
simulation to predict simulated friction and stiffness parameters per foot. The physical decoder transfers to the real world, where it provides self-supervised
labels (within the supervision mask) to train a visual network on real-world image data. In the training stage, the visual network is trained with weak
supervision only on the foothold pixels. In the inference phase, the visual pipeline processes all pixel features within an image and outputs the corresponding
dense prediction of the simulated physical parameters with a confidence mask.
              </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3">Off-road evaluation</h1>
          <div class="item">
            <img src="static/images/fric_hiking.png" alt="Off-road evaluation of dense friction prediction in a hiking scenario."/>
            <h2 class="subtitle has-text-centered">
              Dense friction prediction in a hiking scenario.
            </h2>
          </div>
          <hr>
          <div class="item">
            <img src="static/images/stiff_snow3.png" alt="Off-road evaluation of dense stiffness prediction in a snowy scenario (outdoor)."/>
            <h2 class="subtitle has-text-centered">
              Dense stiffness prediction on outdoor ground covered by heavy snow.
            </h2>
          </div>
          <hr>
          <div class="item">
            <img src="static/images/stiff_snow4.png" alt="Off-road evaluation of dense stiffness prediction in a snowy scenario (indoor)."/>
            <h2 class="subtitle has-text-centered">
              Dense stiffness prediction on indoor rigid ground.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- 
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3">Real-world evaluation (lab)</h1>
          <div class="columns">
            <div class="column is-half">
              <div class="item">
                <img src="static/images/fig8.png" alt="Physical decoder friction estimation. Only left-front (LF) and left-rear (LH) feet are shown in the plot. The Baseline is the decoder in [3]. The robot walks backwards."/>
                <h2 class="subtitle has-text-centered">
                  Physical decoder friction estimation. Only left-front (LF) and left-rear (LH) feet are shown in the plot. The Baseline is the decoder in [3]. The robot walks backwards.
                </h2>
              </div>
            </div>
            <div class="column is-half">
              <div class="item">
                <img src="static/images/fig9.png" alt="Physical decoder stiffness estimation. Left-front (LF) and left-rear (LH) feet are shown in the plot."/>
                <h2 class="subtitle has-text-centered">
                  Physical decoder stiffness estimation. Left-front (LF) and left-rear (LH) feet are shown in the plot.
                </h2>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@INPROCEEDINGS{Chen24physical, 
        AUTHOR    = {Jiaqi Chen AND Jonas Frey AND Ruyi Zhou AND Takahiro Miki AND Georg Martius AND Marco Hutter}, 
        TITLE     = {Identifying Terrain Physical Parameters from Vision - Towards Physical-Parameter-Aware Locomotion and Navigation}, 
        BOOKTITLE = {accepted for IEEE Robotics and Automation Letters (RA-L)}, 
        YEAR      = {2024}
      } </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
